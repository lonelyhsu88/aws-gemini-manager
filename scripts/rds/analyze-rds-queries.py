#!/usr/bin/env python3
"""
RDS Performance Insights Query Analyzer
åˆ†æ RDS å¯¦ä¾‹çš„æŸ¥è©¢æ´»å‹•ï¼Œæ‰¾å‡ºé«˜è² è¼‰æŸ¥è©¢å’Œä¾†æº IP
"""

import boto3
import json
from datetime import datetime, timedelta
from collections import defaultdict
import sys

# AWS é…ç½®
AWS_PROFILE = 'gemini-pro_ck'
REGION = 'ap-east-1'
DB_INSTANCE_ID = 'bingo-prd-backstage-replica1'

# åˆå§‹åŒ– boto3 session
session = boto3.Session(profile_name=AWS_PROFILE, region_name=REGION)
pi_client = session.client('pi')
rds_client = session.client('rds')

def get_db_resource_id(db_instance_id):
    """å–å¾— RDS å¯¦ä¾‹çš„ Resource IDï¼ˆPerformance Insights éœ€è¦ï¼‰"""
    try:
        response = rds_client.describe_db_instances(DBInstanceIdentifier=db_instance_id)
        resource_id = response['DBInstances'][0]['DbiResourceId']
        return resource_id
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•å–å¾— Resource ID - {e}")
        sys.exit(1)

def analyze_performance_insights(resource_id, start_time, end_time):
    """åˆ†æ Performance Insights æ•¸æ“š"""

    print(f"\nğŸ“Š åˆ†ææ™‚æ®µï¼š{start_time.strftime('%Y-%m-%d %H:%M')} ~ {end_time.strftime('%Y-%m-%d %H:%M')} UTC")
    print("=" * 80)

    try:
        # æŸ¥è©¢ Top SQL by DB Load
        response = pi_client.get_resource_metrics(
            ServiceType='RDS',
            Identifier=resource_id,
            MetricQueries=[
                {
                    'Metric': 'db.load.avg',
                    'GroupBy': {
                        'Group': 'db.sql',
                        'Limit': 10
                    }
                }
            ],
            StartTime=start_time,
            EndTime=end_time,
            PeriodInSeconds=3600  # 1å°æ™‚
        )

        print("\nğŸ” Top 10 SQL Queries by Database Load:\n")

        if 'MetricList' in response and len(response['MetricList']) > 0:
            metric_data = response['MetricList'][0]

            if 'DataPoints' in metric_data:
                # æ”¶é›†æ‰€æœ‰æ™‚é–“é»çš„æ•¸æ“š
                sql_stats = defaultdict(lambda: {'total_load': 0, 'max_load': 0, 'count': 0})

                for datapoint in metric_data['DataPoints']:
                    timestamp = datapoint['Timestamp']
                    if 'Dimensions' in datapoint:
                        for dimension in datapoint['Dimensions']:
                            sql_id = dimension.get('Value', 'Unknown')
                            load_value = dimension.get('Limit', 0)

                            sql_stats[sql_id]['total_load'] += load_value
                            sql_stats[sql_id]['max_load'] = max(sql_stats[sql_id]['max_load'], load_value)
                            sql_stats[sql_id]['count'] += 1

                # æ’åºä¸¦é¡¯ç¤º
                sorted_sqls = sorted(sql_stats.items(), key=lambda x: x[1]['total_load'], reverse=True)

                for idx, (sql_id, stats) in enumerate(sorted_sqls[:10], 1):
                    avg_load = stats['total_load'] / stats['count'] if stats['count'] > 0 else 0
                    print(f"{idx}. SQL ID: {sql_id}")
                    print(f"   ç¸½è² è¼‰: {stats['total_load']:.2f}")
                    print(f"   å¹³å‡è² è¼‰: {avg_load:.2f}")
                    print(f"   å³°å€¼è² è¼‰: {stats['max_load']:.2f}")
                    print()

                    # å–å¾— SQL æ–‡æœ¬
                    get_sql_text(resource_id, sql_id)

        # æŸ¥è©¢ Top Waits
        print("\nâ±ï¸  Top Wait Events:\n")
        response_waits = pi_client.get_resource_metrics(
            ServiceType='RDS',
            Identifier=resource_id,
            MetricQueries=[
                {
                    'Metric': 'db.load.avg',
                    'GroupBy': {
                        'Group': 'db.wait_event',
                        'Limit': 10
                    }
                }
            ],
            StartTime=start_time,
            EndTime=end_time,
            PeriodInSeconds=3600
        )

        if 'MetricList' in response_waits and len(response_waits['MetricList']) > 0:
            wait_data = response_waits['MetricList'][0]
            if 'DataPoints' in wait_data:
                wait_stats = defaultdict(float)

                for datapoint in wait_data['DataPoints']:
                    if 'Dimensions' in datapoint:
                        for dimension in datapoint['Dimensions']:
                            wait_event = dimension.get('Value', 'Unknown')
                            load_value = dimension.get('Limit', 0)
                            wait_stats[wait_event] += load_value

                sorted_waits = sorted(wait_stats.items(), key=lambda x: x[1], reverse=True)

                for idx, (wait_event, total_load) in enumerate(sorted_waits[:10], 1):
                    print(f"{idx}. {wait_event}: {total_load:.2f}")

    except Exception as e:
        print(f"âŒ åˆ†æéŒ¯èª¤ï¼š{e}")
        import traceback
        traceback.print_exc()

def get_sql_text(resource_id, sql_id):
    """å–å¾— SQL èªå¥çš„å®Œæ•´æ–‡æœ¬"""
    try:
        response = pi_client.get_dimension_key_details(
            ServiceType='RDS',
            Identifier=resource_id,
            Group='db.sql',
            GroupIdentifier=sql_id
        )

        if 'Dimensions' in response:
            for key, value in response['Dimensions'].items():
                if key == 'db.sql.statement':
                    # æˆªæ–·éé•·çš„ SQL
                    sql_text = value if len(value) <= 200 else value[:200] + '...'
                    print(f"   SQL: {sql_text}")
                    print()
    except Exception as e:
        print(f"   ç„¡æ³•å–å¾— SQL æ–‡æœ¬ï¼š{e}")
        print()

def query_current_connections():
    """
    æ³¨æ„ï¼šé€™å€‹å‡½æ•¸éœ€è¦ç›´æ¥é€£æ¥åˆ°æ•¸æ“šåº«
    éœ€è¦ psycopg2 å¥—ä»¶å’Œæ•¸æ“šåº«æ†‘è­‰
    """
    print("\nğŸ’¡ æç¤ºï¼šè¦æŸ¥è©¢ç•¶å‰é€£æ¥å’Œä¾†æº IPï¼Œéœ€è¦ç›´æ¥é€£æ¥åˆ°æ•¸æ“šåº«")
    print("è«‹åŸ·è¡Œä»¥ä¸‹ SQL æŸ¥è©¢ï¼š")
    print("""
    -- æŸ¥çœ‹ç•¶å‰æ‰€æœ‰é€£æ¥å’Œä¾†æº IP
    SELECT
        pid,
        usename,
        application_name,
        client_addr,
        client_port,
        backend_start,
        state,
        state_change,
        query_start,
        LEFT(query, 100) as query_preview
    FROM pg_stat_activity
    WHERE state != 'idle'
    ORDER BY query_start DESC;

    -- çµ±è¨ˆæ¯å€‹ IP çš„é€£æ¥æ•¸
    SELECT
        client_addr,
        count(*) as connection_count,
        count(*) FILTER (WHERE state = 'active') as active_connections
    FROM pg_stat_activity
    WHERE client_addr IS NOT NULL
    GROUP BY client_addr
    ORDER BY connection_count DESC;
    """)

def main():
    print("\n" + "="*80)
    print("ğŸ” RDS Performance Insights Query Analyzer")
    print("="*80)

    # å–å¾— Resource ID
    resource_id = get_db_resource_id(DB_INSTANCE_ID)
    print(f"\nâœ… RDS Instance: {DB_INSTANCE_ID}")
    print(f"âœ… Resource ID: {resource_id}")

    # åˆ†ææ˜¨å¤©çš„é«˜å³°æ™‚æ®µ (2025-10-29 00:00 - 02:00 UTC)
    # æ ¹æ“šä¹‹å‰çš„æ•¸æ“šï¼Œå•é¡Œç™¼ç”Ÿåœ¨ 00:51 UTC
    start_time = datetime(2025, 10, 29, 0, 0, 0)
    end_time = datetime(2025, 10, 29, 2, 0, 0)

    analyze_performance_insights(resource_id, start_time, end_time)

    # é¡¯ç¤ºå¦‚ä½•æŸ¥è©¢ç•¶å‰é€£æ¥
    query_current_connections()

    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆ")
    print("="*80)

if __name__ == '__main__':
    main()
