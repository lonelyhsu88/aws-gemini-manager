#!/usr/bin/env python3
"""
RDS Database Connections Analysis for bingo-prd
Analyzes DatabaseConnections metric over the past 7 days and provides alarm threshold recommendations.
"""

import boto3
from datetime import datetime, timedelta
import statistics
from collections import defaultdict
import json

# Configuration
AWS_PROFILE = 'gemini-pro_ck'
AWS_REGION = 'ap-east-1'
DB_INSTANCE = 'bingo-prd'
MAX_CONNECTIONS = 901
CURRENT_ALARM = 675
ALARM_PERCENTAGE = 75

def get_cloudwatch_metrics(session, db_instance, days=7):
    """Query CloudWatch for DatabaseConnections metric."""
    cloudwatch = session.client('cloudwatch', region_name=AWS_REGION)

    end_time = datetime.utcnow()
    start_time = end_time - timedelta(days=days)

    print(f"æŸ¥è©¢æ™‚é–“ç¯„åœ: {start_time.strftime('%Y-%m-%d %H:%M:%S')} UTC åˆ° {end_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
    print(f"æŸ¥è©¢è³‡æ–™åº«å¯¦ä¾‹: {db_instance}\n")

    # Get data points with 5-minute period for detailed analysis
    # CloudWatch limit: max 1440 datapoints
    # 7 days * 24 hours * 60 minutes / 5 min period = 2016 datapoints (too many)
    # Using 10-minute period: 7 days * 24 hours * 6 = 1008 datapoints (within limit)
    response = cloudwatch.get_metric_statistics(
        Namespace='AWS/RDS',
        MetricName='DatabaseConnections',
        Dimensions=[
            {
                'Name': 'DBInstanceIdentifier',
                'Value': db_instance
            }
        ],
        StartTime=start_time,
        EndTime=end_time,
        Period=600,  # 10 minutes (7 days = 1008 datapoints, within 1440 limit)
        Statistics=['Average', 'Maximum', 'Minimum']
    )

    datapoints = sorted(response['Datapoints'], key=lambda x: x['Timestamp'])
    return datapoints

def analyze_overall_stats(datapoints):
    """Calculate overall statistics."""
    if not datapoints:
        return None

    averages = [dp['Average'] for dp in datapoints]
    maximums = [dp['Maximum'] for dp in datapoints]
    minimums = [dp['Minimum'] for dp in datapoints]

    return {
        'total_datapoints': len(datapoints),
        'avg_connections': statistics.mean(averages),
        'median_connections': statistics.median(averages),
        'min_connections': min(minimums),
        'max_connections': max(maximums),
        'stddev': statistics.stdev(averages) if len(averages) > 1 else 0,
        'p50': statistics.median(averages),
        'p75': statistics.quantiles(averages, n=4)[2] if len(averages) >= 4 else None,
        'p90': statistics.quantiles(averages, n=10)[8] if len(averages) >= 10 else None,
        'p95': statistics.quantiles(averages, n=20)[18] if len(averages) >= 20 else None,
        'p99': statistics.quantiles(averages, n=100)[98] if len(averages) >= 100 else None,
    }

def analyze_daily_patterns(datapoints):
    """Analyze daily patterns."""
    daily_data = defaultdict(lambda: {'averages': [], 'maximums': [], 'timestamps': []})

    for dp in datapoints:
        # Convert to local time (UTC+8 for Hong Kong)
        local_time = dp['Timestamp'] + timedelta(hours=8)
        date_key = local_time.strftime('%Y-%m-%d')

        daily_data[date_key]['averages'].append(dp['Average'])
        daily_data[date_key]['maximums'].append(dp['Maximum'])
        daily_data[date_key]['timestamps'].append(local_time)

    daily_stats = {}
    for date, data in sorted(daily_data.items()):
        if data['averages']:
            daily_stats[date] = {
                'avg': statistics.mean(data['averages']),
                'max': max(data['maximums']),
                'min': min(data['averages']),
                'peak_time': data['timestamps'][data['maximums'].index(max(data['maximums']))].strftime('%H:%M:%S'),
                'datapoints': len(data['averages']),
                'weekday': data['timestamps'][0].strftime('%A')
            }

    return daily_stats

def analyze_hourly_patterns(datapoints):
    """Analyze hourly patterns to identify peak hours."""
    hourly_data = defaultdict(lambda: {'averages': [], 'maximums': []})

    for dp in datapoints:
        # Convert to local time (UTC+8)
        local_time = dp['Timestamp'] + timedelta(hours=8)
        hour_key = local_time.hour

        hourly_data[hour_key]['averages'].append(dp['Average'])
        hourly_data[hour_key]['maximums'].append(dp['Maximum'])

    hourly_stats = {}
    for hour, data in sorted(hourly_data.items()):
        if data['averages']:
            hourly_stats[hour] = {
                'avg': statistics.mean(data['averages']),
                'max': max(data['maximums']),
                'min': min(data['averages']),
                'datapoints': len(data['averages'])
            }

    return hourly_stats

def analyze_weekly_patterns(datapoints):
    """Analyze weekday vs weekend patterns."""
    weekday_connections = []
    weekend_connections = []

    for dp in datapoints:
        local_time = dp['Timestamp'] + timedelta(hours=8)
        if local_time.weekday() < 5:  # Monday-Friday (0-4)
            weekday_connections.append(dp['Average'])
        else:  # Saturday-Sunday (5-6)
            weekend_connections.append(dp['Average'])

    return {
        'weekday_avg': statistics.mean(weekday_connections) if weekday_connections else 0,
        'weekend_avg': statistics.mean(weekend_connections) if weekend_connections else 0,
        'weekday_max': max(weekday_connections) if weekday_connections else 0,
        'weekend_max': max(weekend_connections) if weekend_connections else 0,
    }

def identify_anomalies(datapoints, threshold_multiplier=2.0):
    """Identify anomalous spikes."""
    if not datapoints:
        return []

    averages = [dp['Average'] for dp in datapoints]
    mean = statistics.mean(averages)
    stddev = statistics.stdev(averages) if len(averages) > 1 else 0
    threshold = mean + (threshold_multiplier * stddev)

    anomalies = []
    for dp in datapoints:
        if dp['Average'] > threshold:
            local_time = dp['Timestamp'] + timedelta(hours=8)
            anomalies.append({
                'timestamp': local_time.strftime('%Y-%m-%d %H:%M:%S'),
                'connections': dp['Average'],
                'deviation': dp['Average'] - mean
            })

    return anomalies

def calculate_alarm_percentile(datapoints, alarm_value):
    """Calculate what percentile the current alarm threshold represents."""
    if not datapoints:
        return None

    averages = sorted([dp['Average'] for dp in datapoints])
    count_below = sum(1 for x in averages if x <= alarm_value)
    percentile = (count_below / len(averages)) * 100

    return percentile

def recommend_thresholds(overall_stats, max_connections):
    """Provide threshold recommendations based on analysis."""
    recommendations = {}

    # P0 (Critical) - Should be very rare, near max capacity
    p0_threshold = min(overall_stats['p99'] * 1.1 if overall_stats['p99'] else max_connections * 0.90,
                       max_connections * 0.90)

    # P1 (High) - Based on P95
    p1_threshold = overall_stats['p95'] * 1.05 if overall_stats['p95'] else max_connections * 0.80

    # P2 (Medium) - Based on P90
    p2_threshold = overall_stats['p90'] * 1.1 if overall_stats['p90'] else max_connections * 0.70

    # P3 (Low/Warning) - Based on P75
    p3_threshold = overall_stats['p75'] * 1.2 if overall_stats['p75'] else max_connections * 0.60

    recommendations['P0_Critical'] = {
        'threshold': int(p0_threshold),
        'percentage': round((p0_threshold / max_connections) * 100, 1),
        'description': 'ç·Šæ€¥å‘Šè­¦ - æ¥è¿‘æœ€å¤§å®¹é‡ï¼Œéœ€ç«‹å³è™•ç†'
    }

    recommendations['P1_High'] = {
        'threshold': int(p1_threshold),
        'percentage': round((p1_threshold / max_connections) * 100, 1),
        'description': 'é«˜å„ªå…ˆç´šå‘Šè­¦ - é€£æ¥æ•¸ç•°å¸¸é«˜ï¼Œéœ€è¦é—œæ³¨'
    }

    recommendations['P2_Medium'] = {
        'threshold': int(p2_threshold),
        'percentage': round((p2_threshold / max_connections) * 100, 1),
        'description': 'ä¸­å„ªå…ˆç´šå‘Šè­¦ - é€£æ¥æ•¸åé«˜ï¼Œå»ºè­°æª¢æŸ¥'
    }

    recommendations['P3_Warning'] = {
        'threshold': int(p3_threshold),
        'percentage': round((p3_threshold / max_connections) * 100, 1),
        'description': 'ä½å„ªå…ˆç´šå‘Šè­¦ - é è­¦é€šçŸ¥'
    }

    return recommendations

def print_report(overall_stats, daily_stats, hourly_stats, weekly_stats, anomalies,
                 alarm_percentile, recommendations):
    """Print comprehensive analysis report."""

    print("=" * 80)
    print("bingo-prd RDS è³‡æ–™åº«é€£æ¥æ•¸åˆ†æå ±å‘Š")
    print("=" * 80)
    print(f"åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"è³‡æ–™åº«å¯¦ä¾‹: {DB_INSTANCE}")
    print(f"å¯¦ä¾‹è¦æ ¼: db.m6g.large")
    print(f"æœ€å¤§é€£æ¥æ•¸: {MAX_CONNECTIONS}")
    print(f"ç•¶å‰å‘Šè­¦é–¾å€¼: {CURRENT_ALARM} ({ALARM_PERCENTAGE}%)")
    print("=" * 80)

    # Overall Statistics
    print("\nğŸ“Š ä¸ƒå¤©æ•´é«”çµ±è¨ˆæ‘˜è¦")
    print("-" * 80)
    print(f"è³‡æ–™é»ç¸½æ•¸: {overall_stats['total_datapoints']:,}")
    print(f"å¹³å‡é€£æ¥æ•¸: {overall_stats['avg_connections']:.2f}")
    print(f"ä¸­ä½æ•¸é€£æ¥æ•¸: {overall_stats['median_connections']:.2f}")
    print(f"æœ€å°é€£æ¥æ•¸: {overall_stats['min_connections']:.2f}")
    print(f"æœ€å¤§é€£æ¥æ•¸: {overall_stats['max_connections']:.2f}")
    print(f"æ¨™æº–å·®: {overall_stats['stddev']:.2f}")
    print(f"\nç™¾åˆ†ä½æ•¸åˆ†æ:")
    print(f"  P50 (ä¸­ä½æ•¸): {overall_stats['p50']:.2f}")
    if overall_stats['p75']:
        print(f"  P75: {overall_stats['p75']:.2f}")
    if overall_stats['p90']:
        print(f"  P90: {overall_stats['p90']:.2f}")
    if overall_stats['p95']:
        print(f"  P95: {overall_stats['p95']:.2f}")
    if overall_stats['p99']:
        print(f"  P99: {overall_stats['p99']:.2f}")

    # Daily Breakdown
    print("\nğŸ“… æ¯æ—¥è©³ç´°åˆ†æ")
    print("-" * 80)
    print(f"{'æ—¥æœŸ':<12} {'æ˜ŸæœŸ':<10} {'å¹³å‡':<10} {'å³°å€¼':<10} {'æœ€ä½':<10} {'å³°å€¼æ™‚é–“':<12}")
    print("-" * 80)
    for date, stats in daily_stats.items():
        print(f"{date:<12} {stats['weekday']:<10} {stats['avg']:>8.2f}  {stats['max']:>8.2f}  "
              f"{stats['min']:>8.2f}  {stats['peak_time']:<12}")

    # Hourly Patterns
    print("\nâ° æ¯å°æ™‚ä½¿ç”¨æ¨¡å¼åˆ†æ")
    print("-" * 80)
    print(f"{'æ™‚æ®µ':<8} {'å¹³å‡é€£æ¥æ•¸':<15} {'å³°å€¼é€£æ¥æ•¸':<15} {'è³‡æ–™é»æ•¸':<10}")
    print("-" * 80)

    # Sort by average to identify peak hours
    sorted_hours = sorted(hourly_stats.items(), key=lambda x: x[1]['avg'], reverse=True)
    for hour, stats in sorted_hours[:24]:  # Show all 24 hours
        print(f"{hour:02d}:00    {stats['avg']:>10.2f}      {stats['max']:>10.2f}      {stats['datapoints']:>8}")

    # Identify peak hours
    peak_hours = sorted_hours[:3]
    print(f"\nğŸ”¥ æœ€ç¹å¿™æ™‚æ®µ (Top 3):")
    for hour, stats in peak_hours:
        print(f"   {hour:02d}:00 - å¹³å‡ {stats['avg']:.2f} é€£æ¥ï¼Œå³°å€¼ {stats['max']:.2f}")

    # Weekly Patterns
    print("\nğŸ“ˆ å·¥ä½œæ—¥ vs é€±æœ«åˆ†æ")
    print("-" * 80)
    print(f"å·¥ä½œæ—¥å¹³å‡: {weekly_stats['weekday_avg']:.2f} é€£æ¥")
    print(f"é€±æœ«å¹³å‡: {weekly_stats['weekend_avg']:.2f} é€£æ¥")
    print(f"å·¥ä½œæ—¥å³°å€¼: {weekly_stats['weekday_max']:.2f} é€£æ¥")
    print(f"é€±æœ«å³°å€¼: {weekly_stats['weekend_max']:.2f} é€£æ¥")

    diff_percentage = ((weekly_stats['weekday_avg'] - weekly_stats['weekend_avg']) /
                       weekly_stats['weekend_avg'] * 100) if weekly_stats['weekend_avg'] > 0 else 0
    if abs(diff_percentage) > 10:
        if diff_percentage > 0:
            print(f"\nğŸ’¡ è§€å¯Ÿ: å·¥ä½œæ—¥æµé‡æ¯”é€±æœ«é«˜ {abs(diff_percentage):.1f}%")
        else:
            print(f"\nğŸ’¡ è§€å¯Ÿ: é€±æœ«æµé‡æ¯”å·¥ä½œæ—¥é«˜ {abs(diff_percentage):.1f}%")
    else:
        print(f"\nğŸ’¡ è§€å¯Ÿ: å·¥ä½œæ—¥èˆ‡é€±æœ«æµé‡ç›¸è¿‘ (å·®ç•° {abs(diff_percentage):.1f}%)")

    # Anomalies
    if anomalies:
        print("\nâš ï¸  ç•°å¸¸å³°å€¼æª¢æ¸¬ (è¶…éå¹³å‡å€¼ + 2å€æ¨™æº–å·®)")
        print("-" * 80)
        print(f"æª¢æ¸¬åˆ° {len(anomalies)} å€‹ç•°å¸¸å³°å€¼:")
        for anomaly in anomalies[:10]:  # Show top 10
            print(f"  {anomaly['timestamp']} - {anomaly['connections']:.2f} é€£æ¥ "
                  f"(åé›¢ +{anomaly['deviation']:.2f})")
        if len(anomalies) > 10:
            print(f"  ... åŠå…¶ä»– {len(anomalies) - 10} å€‹ç•°å¸¸é»")
    else:
        print("\nâœ… æœªæª¢æ¸¬åˆ°é¡¯è‘—ç•°å¸¸å³°å€¼")

    # Current Alarm Evaluation
    print("\nğŸ¯ ç•¶å‰å‘Šè­¦é–¾å€¼è©•ä¼°")
    print("-" * 80)
    print(f"ç•¶å‰è¨­å®š: {CURRENT_ALARM} é€£æ¥ ({ALARM_PERCENTAGE}%)")
    if alarm_percentile:
        print(f"ç™¾åˆ†ä½æ•¸: P{alarm_percentile:.1f}")
        print(f"è§£é‡‹: ç•¶å‰é–¾å€¼é«˜æ–¼ {alarm_percentile:.1f}% çš„è§€æ¸¬å€¼")

        if alarm_percentile > 95:
            print(f"è©•ä¼°: âš ï¸  é–¾å€¼è¨­å®šåé«˜ï¼Œå¯èƒ½æœƒéŒ¯éé‡è¦å‘Šè­¦")
        elif alarm_percentile > 85:
            print(f"è©•ä¼°: âœ… é–¾å€¼è¨­å®šåˆç†ï¼Œèƒ½æœ‰æ•ˆæ•æ‰ç•°å¸¸æƒ…æ³")
        else:
            print(f"è©•ä¼°: âš ï¸  é–¾å€¼è¨­å®šåä½ï¼Œå¯èƒ½ç”¢ç”Ÿéå¤šå‘Šè­¦")

    # Trends Analysis
    print("\nğŸ“Š è¶¨å‹¢åˆ†æ")
    print("-" * 80)

    # Calculate trend from first 2 days vs last 2 days
    dates = sorted(daily_stats.keys())
    if len(dates) >= 4:
        early_avg = statistics.mean([daily_stats[d]['avg'] for d in dates[:2]])
        late_avg = statistics.mean([daily_stats[d]['avg'] for d in dates[-2:]])
        trend_change = ((late_avg - early_avg) / early_avg * 100) if early_avg > 0 else 0

        if abs(trend_change) < 5:
            print(f"ä½¿ç”¨é‡è¶¨å‹¢: ç©©å®š (è®ŠåŒ– {trend_change:+.1f}%)")
            print("ğŸ’¡ å»ºè­°: ç•¶å‰å®¹é‡è¦åŠƒé©ç•¶")
        elif trend_change > 0:
            print(f"ä½¿ç”¨é‡è¶¨å‹¢: ä¸Šå‡ (å¢åŠ  {trend_change:.1f}%)")
            print("ğŸ’¡ å»ºè­°: éœ€è¦ç›£æ§å¢é•·è¶¨å‹¢ï¼Œè©•ä¼°æ˜¯å¦éœ€è¦æ“´å®¹")
        else:
            print(f"ä½¿ç”¨é‡è¶¨å‹¢: ä¸‹é™ (æ¸›å°‘ {abs(trend_change):.1f}%)")
            print("ğŸ’¡ å»ºè­°: å¯è€ƒæ…®å„ªåŒ–è³‡æºé…ç½®")

    # Recommendations
    print("\nğŸ’¡ å‘Šè­¦é–¾å€¼å»ºè­°")
    print("=" * 80)

    for level, rec in recommendations.items():
        print(f"\n{level}:")
        print(f"  é–¾å€¼: {rec['threshold']} é€£æ¥ ({rec['percentage']}%)")
        print(f"  èªªæ˜: {rec['description']}")

    # Capacity Planning
    print("\nğŸ“ˆ å®¹é‡è¦åŠƒå»ºè­°")
    print("=" * 80)

    utilization = (overall_stats['max_connections'] / MAX_CONNECTIONS) * 100
    print(f"ç•¶å‰æœ€å¤§ä½¿ç”¨ç‡: {utilization:.1f}%")

    if utilization > 80:
        print("âš ï¸  è­¦å‘Š: å³°å€¼ä½¿ç”¨ç‡è¶…é 80%ï¼Œå»ºè­°è€ƒæ…®å‡ç´šå¯¦ä¾‹")
        print("   å»ºè­°: å‡ç´šè‡³ db.m6g.xlarge æˆ–å„ªåŒ–æ‡‰ç”¨é€£æ¥æ± ")
    elif utilization > 60:
        print("âš ï¸  æ³¨æ„: å³°å€¼ä½¿ç”¨ç‡è¶…é 60%ï¼Œéœ€è¦æŒçºŒç›£æ§")
        print("   å»ºè­°: å„ªåŒ–æ‡‰ç”¨é€£æ¥æ± é…ç½®ï¼Œæ¸›å°‘ä¸å¿…è¦çš„é€£æ¥")
    else:
        print("âœ… è‰¯å¥½: ç•¶å‰å®¹é‡å……è¶³")

    # Final Summary
    print("\n" + "=" * 80)
    print("ç¸½çµå»ºè­°")
    print("=" * 80)

    print(f"\n1. å‘Šè­¦é…ç½®å»ºè­°:")
    print(f"   - å»ºè­°å°‡ä¸»è¦å‘Šè­¦é–¾å€¼å¾ {CURRENT_ALARM} èª¿æ•´ç‚º {recommendations['P1_High']['threshold']}")
    print(f"   - é…ç½®å¤šå±¤ç´šå‘Šè­¦: P3({recommendations['P3_Warning']['threshold']}) -> "
          f"P2({recommendations['P2_Medium']['threshold']}) -> "
          f"P1({recommendations['P1_High']['threshold']}) -> "
          f"P0({recommendations['P0_Critical']['threshold']})")

    print(f"\n2. ç›£æ§é‡é»æ™‚æ®µ:")
    peak_hours_str = ", ".join([f"{h:02d}:00" for h, _ in peak_hours])
    print(f"   - é‡é»ç›£æ§: {peak_hours_str}")

    print(f"\n3. è¡Œå‹•å»ºè­°:")
    if utilization > 70:
        print(f"   - å„ªå…ˆ: æª¢æŸ¥æ‡‰ç”¨ç¨‹å¼é€£æ¥æ± é…ç½®")
        print(f"   - å„ªå…ˆ: èª¿æŸ¥æ˜¯å¦æœ‰é€£æ¥æ´©æ¼å•é¡Œ")
        print(f"   - è€ƒæ…®: è©•ä¼°å¯¦ä¾‹å‡ç´šéœ€æ±‚")
    else:
        print(f"   - æŒçºŒç›£æ§é€£æ¥æ•¸è¶¨å‹¢")
        print(f"   - å®šæœŸæª¢æŸ¥æ‡‰ç”¨é€£æ¥ä½¿ç”¨æ•ˆç‡")

    print("\n" + "=" * 80)

def main():
    """Main execution function."""
    try:
        # Initialize AWS session
        session = boto3.Session(profile_name=AWS_PROFILE)

        # Get CloudWatch metrics
        print("æ­£åœ¨æŸ¥è©¢ CloudWatch æŒ‡æ¨™æ•¸æ“š...")
        datapoints = get_cloudwatch_metrics(session, DB_INSTANCE, days=7)

        if not datapoints:
            print("âŒ éŒ¯èª¤: æœªèƒ½ç²å– CloudWatch æ•¸æ“š")
            print("è«‹ç¢ºèª:")
            print(f"  1. è³‡æ–™åº«å¯¦ä¾‹åç¨±æ­£ç¢º: {DB_INSTANCE}")
            print(f"  2. AWS Region æ­£ç¢º: {AWS_REGION}")
            print(f"  3. AWS Profile æœ‰æ¬Šé™è¨ªå• CloudWatch")
            return

        print(f"âœ… æˆåŠŸç²å– {len(datapoints)} å€‹æ•¸æ“šé»\n")

        # Perform analysis
        overall_stats = analyze_overall_stats(datapoints)
        daily_stats = analyze_daily_patterns(datapoints)
        hourly_stats = analyze_hourly_patterns(datapoints)
        weekly_stats = analyze_weekly_patterns(datapoints)
        anomalies = identify_anomalies(datapoints)
        alarm_percentile = calculate_alarm_percentile(datapoints, CURRENT_ALARM)
        recommendations = recommend_thresholds(overall_stats, MAX_CONNECTIONS)

        # Print comprehensive report
        print_report(overall_stats, daily_stats, hourly_stats, weekly_stats,
                    anomalies, alarm_percentile, recommendations)

        # Save raw data for further analysis
        output_file = f'/Users/lonelyhsu/gemini/claude-project/aws-gemini-manager/scripts/rds/bingo-prd-analysis-{datetime.now().strftime("%Y%m%d-%H%M%S")}.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'analysis_time': datetime.now().isoformat(),
                'db_instance': DB_INSTANCE,
                'max_connections': MAX_CONNECTIONS,
                'current_alarm': CURRENT_ALARM,
                'overall_stats': overall_stats,
                'daily_stats': daily_stats,
                'hourly_stats': {str(k): v for k, v in hourly_stats.items()},
                'weekly_stats': weekly_stats,
                'anomalies': anomalies,
                'alarm_percentile': alarm_percentile,
                'recommendations': recommendations
            }, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… è©³ç´°æ•¸æ“šå·²ä¿å­˜è‡³: {output_file}")

    except Exception as e:
        print(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
